{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6eaee2fe-9dd7-4d9f-828c-148f8b257eea",
   "metadata": {},
   "source": [
    "# Langchain RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d639468-9256-4db4-8ddf-75c1efbddd46",
   "metadata": {},
   "source": [
    "A typical RAG application has two main components:\n",
    "\n",
    "**Indexing**: a pipeline for ingesting data from a source and indexing it. This usually happens offline.\n",
    "\n",
    "**Retrieval and generation**: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\n",
    "\n",
    "The most common full sequence from raw data to answer looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5335f758-3e24-44ae-93c6-3adfe1e1e030",
   "metadata": {},
   "source": [
    "### Indexing\n",
    "1. **Load**: First we need to load our data. This is done in langchain with Document Loaders classes.\n",
    "2. **Split**: Text splitters break large Documents into smaller chunks. This is necessary because embedding models have a finite context window.\n",
    "3. **Embed**: Then we need to convert those chunks into vectors. This is done with an embedding model.\n",
    "4. **Store**: We need somewhere to store and index our vectors from the text chunks, so that we can search over them later. This is done using a VectorStore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ae9c94-2f9f-4f6e-98c1-0f3fb00bb0d0",
   "metadata": {},
   "source": [
    "### Retrieval and generation\n",
    "5. **Retrieve**: Given a user input, relevant splits are retrieved from storage using a Retriever.\n",
    "6. **Generate**: A LLM produces an answer using a prompt that includes both the question and the retrieved data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319ead7f-f0b0-422d-8dfa-0f67e8f6e80b",
   "metadata": {},
   "source": [
    "![Indexing pipeline](./indexing-pipeline.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce420411-1260-4bdf-9fc5-666d29ca9f8a",
   "metadata": {},
   "source": [
    "Prepare to import your API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ae3b5ba-fd14-4ecc-8e4c-8100e21e9e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /Users/user001/anaconda3/lib/python3.11/site-packages (0.21.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1ccaf7-b1d0-4e96-8c89-833ed72fb7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-openai in /Users/user001/anaconda3/lib/python3.11/site-packages (0.3.16)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /Users/user001/anaconda3/lib/python3.11/site-packages (from langchain-openai) (0.3.58)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /Users/user001/anaconda3/lib/python3.11/site-packages (from langchain-openai) (1.77.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /Users/user001/anaconda3/lib/python3.11/site-packages (from langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /Users/user001/anaconda3/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain-openai) (0.3.42)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/user001/anaconda3/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain-openai) (8.2.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/user001/anaconda3/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain-openai) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/user001/anaconda3/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain-openai) (6.0)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/user001/anaconda3/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain-openai) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/user001/anaconda3/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain-openai) (4.13.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /Users/user001/anaconda3/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain-openai) (2.11.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/user001/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (3.5.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/user001/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/user001/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/user001/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: sniffio in /Users/user001/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.2.0)\n",
      "Requirement already satisfied: tqdm>4 in /Users/user001/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.65.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/user001/anaconda3/lib/python3.11/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2022.7.9)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/user001/anaconda3/lib/python3.11/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2.31.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/user001/anaconda3/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.68.2->langchain-openai) (3.4)\n",
      "Requirement already satisfied: certifi in /Users/user001/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/user001/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/user001/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/user001/anaconda3/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain-openai) (2.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/user001/anaconda3/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.58->langchain-openai) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/user001/anaconda3/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.58->langchain-openai) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /Users/user001/anaconda3/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.58->langchain-openai) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/user001/anaconda3/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.58->langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /Users/user001/anaconda3/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.58->langchain-openai) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/user001/anaconda3/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.58->langchain-openai) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/user001/anaconda3/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/user001/anaconda3/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (1.26.16)\n"
     ]
    }
   ],
   "source": [
    "#pip install langchain\n",
    "!pip install -U langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe95504-6ec4-4adf-840b-43a170a7656e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(os.path.expanduser(\"~/Projekte/MOOC/OpenCampus/codespace/.env\"))\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  # os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "    raise EnvironmentError(\"OPENAI_API_KEY not found in the .env file.\")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2c3baa-a26c-4302-8531-ab8777663822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882c2e35-cd7f-4657-aa8d-cf74b863a2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b8f1b5-3aa5-4cd8-bd4b-95d3fc21ed53",
   "metadata": {},
   "source": [
    "In this guide we’ll build an app that answers questions about the website's content. The specific website we will use is the [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) blog post by Lilian Weng, which allows us to ask questions about the contents of the post.\n",
    "\n",
    "We can create a simple indexing pipeline and RAG chain to do this in ~50 lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de32ae1-6827-454a-b1c0-6498274d3010",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-text-splitters langchain-community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce81afbb-afc6-4de0-bd0e-1f236cbf9f7e",
   "metadata": {},
   "source": [
    " Load & Split Your Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e663a7-3811-4a33-a216-61fab567a6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca7b322-f4cd-444d-b25d-6a5e5794845a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and chunk contents of the blog\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb1acff-8273-4097-8dcc-305ae133dbfe",
   "metadata": {},
   "source": [
    "Embed & Store Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99292a59-694b-4ae8-8c75-ed093549d383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index chunks\n",
    "_ = vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3138907f-08c6-4dca-8c3c-48d97d7d9927",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_type = \"similarity\",     # \"mmr\" or \"similarity_score_threshold\" also work\n",
    "    search_kwargs = {\"k\": 4}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71ca238-266a-467b-8299-80641c0d23bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "SYSTEM = \"\"\"You are an expert assistant.\n",
    "Answer *only* from the context between <context></context>;\n",
    "if the answer isn’t there, say “I don't know.”\"\"\"\n",
    "USER = \"\"\"<context>\\n{context}\\n</context>\\n\\nQuestion: {input}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", SYSTEM), (\"user\", USER)])\n",
    "\n",
    "# NEW: wrap llm + prompt in a \"stuff-documents\" chain → Runnable\n",
    "combine_docs_chain = create_stuff_documents_chain(llm, prompt)  \n",
    "\n",
    "# Build the final RAG runnable\n",
    "rag_chain = create_retrieval_chain(retriever, combine_docs_chain) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7410e8-75ad-4982-a009-1e9f68ad021a",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is Task Decomposition?\"\n",
    "result   = rag_chain.invoke({\"input\": question})\n",
    "\n",
    "print(result[\"answer\"])      # grounded answer\n",
    "print(result[\"context\"])     # the stuffed context string (if you need sources, see below)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09dfc3e-d18e-4e9e-8b43-0977c02933ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
