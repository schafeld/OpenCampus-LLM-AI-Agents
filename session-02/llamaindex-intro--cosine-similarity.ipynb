{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25c75732-3e10-48c9-89d3-dd43595bbc3d",
   "metadata": {},
   "source": [
    "# First llamaindex tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b5a67c-fc98-44c2-b534-b20025276e51",
   "metadata": {},
   "source": [
    "Prepare to import your API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77edabf7-09fb-4f4c-9930-f75b522ef2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /Users/user001/anaconda3/lib/python3.11/site-packages (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7865bf-2c6d-4fb0-84a9-d6bb4ffd2f5a",
   "metadata": {},
   "source": [
    "Similar to langchain llamaindex provides a lot of functionality to easily work with LLMs. It is one of the most popular frameworks besides langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53499eb2-dacd-4395-ba3b-e413492d1444",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c39a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#skip this section if you have no openai api key\n",
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(os.path.expanduser(\"~/Projekte/MOOC/OpenCampus/codespace/.env\"))\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\n",
    "        prompt=\"Enter your OpenAI API key (required if using OpenAI): \"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee07feb-7ee6-4a6f-8531-7f7770ed6671",
   "metadata": {},
   "source": [
    "Let's ask the model about if it knows about opencampus.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7100bf1a-c1bc-4928-8bba-8e71f580aac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "response = OpenAI(model=\"gpt-4o-mini\").complete(\"What is opencampus.sh \")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6c12ad-393a-4f24-802a-a2da23f9b900",
   "metadata": {},
   "source": [
    "Uhh that is kind of a bummer, it does not even know about us. Probably because opencampus.sh is only fewly mentioned on any website etc.\n",
    "\n",
    "Let's try out the new 4.1 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76759e34-856d-4e8e-8a1e-2c9b7f65f91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "llm = OpenAI(model=\"gpt-4.1-mini\")\n",
    "messages = [\n",
    "    ChatMessage(role=\"system\", content=\"You are a helpful assistant.\"),\n",
    "    ChatMessage(role=\"user\", content=\"What is opencampus.sh\"),\n",
    "]\n",
    "chat_response = llm.chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3281939d-6c07-46f0-bd01-b647e8759e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b1b618-d33f-45fc-8f6e-70283488fbb0",
   "metadata": {},
   "source": [
    "Hmm seems to go a bit into hallucinations here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681ec06c-f4df-4894-8c06-69527c03494c",
   "metadata": {},
   "source": [
    "## Olama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0033ab4b-9ff1-43d9-ad66-32651fd65c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install llama-index-llms-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2e6417-107c-449e-8cf1-9db4a981348f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "llm = Ollama(model=\"gemma3:1b\", request_timeout=60.0)\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(role=\"system\", content=\"You are a helpful assistant.\"),\n",
    "    ChatMessage(role=\"user\", content=\"What is opencampus.sh\"),\n",
    "]\n",
    "\n",
    "chat_response = llm.chat(messages)\n",
    "print(chat_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0149a62e-d2ca-4d70-82cc-586e7c1baafe",
   "metadata": {},
   "source": [
    "Oh dear.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1a71c5-25ce-4d7a-8bc8-82a2c65bd465",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We have libraries which wrap the llm call into functions and classes of their liking. We gain that we can easily switch between different model providers but loose at lot of flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007a9231-8ae6-4e3b-8098-1e3b24cac183",
   "metadata": {},
   "source": [
    "Made with GitHub Copilot\n",
    "# Adding RAG with Cosine Similarity to the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60bf4ef-f98d-4f91-8cab-8a935c163fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional required packages\n",
    "#!pip install llama-index-vector-stores-simple numpy\n",
    "!pip install llama-index numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a77b14-1c66-41be-8436-a47d6a9f0422",
   "metadata": {},
   "source": [
    "Then, let's add code to load documents and create an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa85cf75-f641-499a-9b00-b47adb5452d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory with some sample documents about OpenCampus.sh\n",
    "import os\n",
    "\n",
    "# Create a documents directory if it doesn't exist\n",
    "os.makedirs(\"documents\", exist_ok=True)\n",
    "\n",
    "# Create a sample document about OpenCampus.sh\n",
    "with open(\"documents/opencampus_info.txt\", \"w\") as f:\n",
    "    f.write(\"\"\"\n",
    "OpenCampus.sh is an innovative educational initiative based in Hamburg, Germany. \n",
    "It offers a variety of open and accessible learning opportunities in tech, \n",
    "digital skills, and modern professions. The platform provides MOOCs (Massive Open Online Courses), \n",
    "workshops, and project-based learning experiences that are often free or low-cost.\n",
    "\n",
    "Key characteristics of OpenCampus.sh:\n",
    "1. Community-focused education model\n",
    "2. Collaboration with industry partners and universities\n",
    "3. Practical, hands-on learning experiences\n",
    "4. Focus on modern digital skills and technologies\n",
    "5. Based in Hamburg but with online course offerings\n",
    "\n",
    "OpenCampus.sh aims to democratize access to education and bridge the gap between academic \n",
    "knowledge and practical industry requirements in the digital age.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ab6ae8-2c2e-4712-a21d-c9144efea782",
   "metadata": {},
   "source": [
    "Now let's implement document loading and indexing with cosine similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c24ac1-96de-4f55-9cbb-526b39bf9f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "# from llama_index.vector_stores.simple import SimpleVectorStore\n",
    "from llama_index.core.vector_stores.simple import SimpleVectorStore\n",
    "from llama_index.core.vector_stores import MetadataFilters\n",
    "from llama_index.core.vector_stores import FilterOperator, ExactMatchFilter\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "import numpy as np\n",
    "\n",
    "# Load documents\n",
    "documents = SimpleDirectoryReader(\"documents\").load_data()\n",
    "print(f\"Loaded {len(documents)} documents\")\n",
    "\n",
    "# Create embedding model\n",
    "embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Create a vector store with cosine similarity as the distance function\n",
    "vector_store = SimpleVectorStore(similarity_top_k=2, vector_store_type=\"dict\", distance_fn=\"cosine\")\n",
    "\n",
    "# Create and populate the index\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    embed_model=embed_model,\n",
    "    vector_store=vector_store,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "# Save the index\n",
    "index.storage_context.persist(\"index_cosine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d065332-471c-424a-8e90-6e6922923fad",
   "metadata": {},
   "source": [
    "Now let's add a function to perform RAG with cosine similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc4ddba-0508-4034-80c7-9bb1041c13b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "\n",
    "# Create a retriever with cosine similarity\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=2,\n",
    ")\n",
    "\n",
    "# Create a query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    llm=OpenAI(model=\"gpt-4o-mini\")\n",
    ")\n",
    "\n",
    "# Now let's query about OpenCampus.sh with our RAG system\n",
    "response = query_engine.query(\"What is OpenCampus.sh and what does it do?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b48cb5-7809-4e06-8299-bb98f2910226",
   "metadata": {},
   "source": [
    "Let's also add a function to demonstrate how cosine similarity works under the hood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffffc566-cf5c-40cc-a158-d8260ef0ad76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_demo():\n",
    "    \"\"\"Demonstrate how cosine similarity works with embeddings\"\"\"\n",
    "    # Get embeddings for two related texts\n",
    "    text1 = \"OpenCampus.sh provides educational opportunities.\"\n",
    "    text2 = \"Education and learning are offered by OpenCampus.\"\n",
    "    text3 = \"Hamburg is a city in northern Germany.\"\n",
    "    \n",
    "    # Get embeddings\n",
    "    embedding1 = embed_model.get_text_embedding(text1)\n",
    "    embedding2 = embed_model.get_text_embedding(text2)\n",
    "    embedding3 = embed_model.get_text_embedding(text3)\n",
    "    \n",
    "    # Convert to numpy arrays for easier calculation\n",
    "    emb1 = np.array(embedding1)\n",
    "    emb2 = np.array(embedding2)\n",
    "    emb3 = np.array(embedding3)\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    def cosine_sim(a, b):\n",
    "        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    \n",
    "    sim_1_2 = cosine_sim(emb1, emb2)\n",
    "    sim_1_3 = cosine_sim(emb1, emb3)\n",
    "    sim_2_3 = cosine_sim(emb2, emb3)\n",
    "    \n",
    "    print(f\"Similarity between related texts: {sim_1_2:.4f}\")\n",
    "    print(f\"Similarity between text1 and unrelated text: {sim_1_3:.4f}\")\n",
    "    print(f\"Similarity between text2 and unrelated text: {sim_2_3:.4f}\")\n",
    "    \n",
    "    return \"As shown, cosine similarity gives higher scores to semantically related texts\"\n",
    "\n",
    "# Run the demonstration\n",
    "cosine_similarity_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a2a8fd-ee52-46a6-a380-6384baf3849b",
   "metadata": {},
   "source": [
    "Finally, let's add a cell to compare RAG results with the previous direct LLM queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e3f8c7-f820-4fed-bc5f-fbdabb6da07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare RAG results with direct LLM query\n",
    "print(\"--- Direct LLM Query ---\")\n",
    "direct_response = OpenAI(model=\"gpt-4o-mini\").complete(\"What is OpenCampus.sh?\")\n",
    "print(direct_response)\n",
    "\n",
    "print(\"\\n--- RAG with Cosine Similarity ---\")\n",
    "rag_response = query_engine.query(\"What is OpenCampus.sh?\")\n",
    "print(rag_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
