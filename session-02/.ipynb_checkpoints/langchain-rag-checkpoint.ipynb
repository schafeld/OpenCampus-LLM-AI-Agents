{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6eaee2fe-9dd7-4d9f-828c-148f8b257eea",
   "metadata": {},
   "source": [
    "# Langchain RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d639468-9256-4db4-8ddf-75c1efbddd46",
   "metadata": {},
   "source": [
    "A typical RAG application has two main components:\n",
    "\n",
    "**Indexing**: a pipeline for ingesting data from a source and indexing it. This usually happens offline.\n",
    "\n",
    "**Retrieval and generation**: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\n",
    "\n",
    "The most common full sequence from raw data to answer looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5335f758-3e24-44ae-93c6-3adfe1e1e030",
   "metadata": {},
   "source": [
    "### Indexing\n",
    "1. **Load**: First we need to load our data. This is done in langchain with Document Loaders classes.\n",
    "2. **Split**: Text splitters break large Documents into smaller chunks. This is necessary because embedding models have a finite context window.\n",
    "3. **Embed**: Then we need to convert those chunks into vectors. This is done with an embedding model.\n",
    "4. **Store**: We need somewhere to store and index our vectors from the text chunks, so that we can search over them later. This is done using a VectorStore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ae9c94-2f9f-4f6e-98c1-0f3fb00bb0d0",
   "metadata": {},
   "source": [
    "### Retrieval and generation\n",
    "5. **Retrieve**: Given a user input, relevant splits are retrieved from storage using a Retriever.\n",
    "6. **Generate**: A LLM produces an answer using a prompt that includes both the question and the retrieved data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319ead7f-f0b0-422d-8dfa-0f67e8f6e80b",
   "metadata": {},
   "source": [
    "![Indexing pipeline](./indexing-pipeline.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfe95504-6ec4-4adf-840b-43a170a7656e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c2c3baa-a26c-4302-8531-ab8777663822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "882c2e35-cd7f-4657-aa8d-cf74b863a2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b8f1b5-3aa5-4cd8-bd4b-95d3fc21ed53",
   "metadata": {},
   "source": [
    "In this guide we’ll build an app that answers questions about the website's content. The specific website we will use is the [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) blog post by Lilian Weng, which allows us to ask questions about the contents of the post.\n",
    "\n",
    "We can create a simple indexing pipeline and RAG chain to do this in ~50 lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7de32ae1-6827-454a-b1c0-6498274d3010",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install langchain-text-splitters langchain-community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce81afbb-afc6-4de0-bd0e-1f236cbf9f7e",
   "metadata": {},
   "source": [
    " Load & Split Your Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0e663a7-3811-4a33-a216-61fab567a6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca7b322-f4cd-444d-b25d-6a5e5794845a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and chunk contents of the blog\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb1acff-8273-4097-8dcc-305ae133dbfe",
   "metadata": {},
   "source": [
    "Embed & Store Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99292a59-694b-4ae8-8c75-ed093549d383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index chunks\n",
    "_ = vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3138907f-08c6-4dca-8c3c-48d97d7d9927",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_type = \"similarity\",     # \"mmr\" or \"similarity_score_threshold\" also work\n",
    "    search_kwargs = {\"k\": 4}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d71ca238-266a-467b-8299-80641c0d23bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "SYSTEM = \"\"\"You are an expert assistant.\n",
    "Answer *only* from the context between <context></context>;\n",
    "if the answer isn’t there, say “I don't know.”\"\"\"\n",
    "USER = \"\"\"<context>\\n{context}\\n</context>\\n\\nQuestion: {input}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", SYSTEM), (\"user\", USER)])\n",
    "\n",
    "# NEW: wrap llm + prompt in a \"stuff-documents\" chain → Runnable\n",
    "combine_docs_chain = create_stuff_documents_chain(llm, prompt)  \n",
    "\n",
    "# Build the final RAG runnable\n",
    "rag_chain = create_retrieval_chain(retriever, combine_docs_chain) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd7410e8-75ad-4982-a009-1e9f68ad021a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Decomposition is a process where a complicated task is broken down into smaller and simpler steps. This can be achieved through techniques such as Chain of Thought (CoT), which instructs the model to \"think step by step\" to utilize more test-time computation and decompose hard tasks. Additionally, Tree of Thoughts extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thought steps. Task decomposition can also be done using simple prompting, task-specific instructions, or human inputs.\n",
      "[Document(id='d5884288-eb21-477c-99b2-581867d86ec8', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.'), Document(id='5c9a2c43-6847-4b83-a89c-8ba09fc687be', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'), Document(id='7784ad06-659d-4e21-874e-aaca4d2a2721', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.'), Document(id='8eb40dfb-d354-4a82-801d-1cd6b676439f', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:')]\n"
     ]
    }
   ],
   "source": [
    "question = \"What is Task Decomposition?\"\n",
    "result   = rag_chain.invoke({\"input\": question})\n",
    "\n",
    "print(result[\"answer\"])      # grounded answer\n",
    "print(result[\"context\"])     # the stuffed context string (if you need sources, see below)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09dfc3e-d18e-4e9e-8b43-0977c02933ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
