{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6eaee2fe-9dd7-4d9f-828c-148f8b257eea",
   "metadata": {},
   "source": [
    "# Langchain RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d639468-9256-4db4-8ddf-75c1efbddd46",
   "metadata": {},
   "source": [
    "A typical RAG application has two main components:\n",
    "\n",
    "**Indexing**: a pipeline for ingesting data from a source and indexing it. This usually happens offline.\n",
    "\n",
    "**Retrieval and generation**: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\n",
    "\n",
    "The most common full sequence from raw data to answer looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5335f758-3e24-44ae-93c6-3adfe1e1e030",
   "metadata": {},
   "source": [
    "### Indexing\n",
    "1. **Load**: First we need to load our data. This is done in langchain with Document Loaders classes.\n",
    "2. **Split**: Text splitters break large Documents into smaller chunks. This is necessary because embedding models have a finite context window.\n",
    "3. **Embed**: Then we need to convert those chunks into vectors. This is done with an embedding model.\n",
    "4. **Store**: We need somewhere to store and index our vectors from the text chunks, so that we can search over them later. This is done using a VectorStore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ae9c94-2f9f-4f6e-98c1-0f3fb00bb0d0",
   "metadata": {},
   "source": [
    "### Retrieval and generation\n",
    "5. **Retrieve**: Given a user input, relevant splits are retrieved from storage using a Retriever.\n",
    "6. **Generate**: A LLM produces an answer using a prompt that includes both the question and the retrieved data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319ead7f-f0b0-422d-8dfa-0f67e8f6e80b",
   "metadata": {},
   "source": [
    "![Indexing pipeline](./indexing-pipeline.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce420411-1260-4bdf-9fc5-666d29ca9f8a",
   "metadata": {},
   "source": [
    "Prepare to import your API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ae3b5ba-fd14-4ecc-8e4c-8100e21e9e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /Users/user001/anaconda3/lib/python3.11/site-packages (0.21.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfe95504-6ec4-4adf-840b-43a170a7656e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m----> 5\u001b[0m   os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m getpass\u001b[38;5;241m.\u001b[39mgetpass(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter API key for OpenAI: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_chat_model\n\u001b[1;32m      9\u001b[0m llm \u001b[38;5;241m=\u001b[39m init_chat_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o-mini\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_provider\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py:1185\u001b[0m, in \u001b[0;36mKernel.getpass\u001b[0;34m(self, prompt, stream)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `stream` parameter of `getpass.getpass` will have no effect when using ipykernel\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m   1183\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   1184\u001b[0m     )\n\u001b[0;32m-> 1185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[1;32m   1186\u001b[0m     prompt,\n\u001b[1;32m   1187\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1188\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1189\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1190\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py:1245\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1243\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1244\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "load_dotenv(os.path.expanduser(\"~/Projekte/MOOC/OpenCampus/codespace/.env\"))\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  # os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "    raise EnvironmentError(\"OPENAI_API_KEY not found in the .env file.\")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2c3baa-a26c-4302-8531-ab8777663822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882c2e35-cd7f-4657-aa8d-cf74b863a2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b8f1b5-3aa5-4cd8-bd4b-95d3fc21ed53",
   "metadata": {},
   "source": [
    "In this guide we’ll build an app that answers questions about the website's content. The specific website we will use is the [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) blog post by Lilian Weng, which allows us to ask questions about the contents of the post.\n",
    "\n",
    "We can create a simple indexing pipeline and RAG chain to do this in ~50 lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de32ae1-6827-454a-b1c0-6498274d3010",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install langchain-text-splitters langchain-community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce81afbb-afc6-4de0-bd0e-1f236cbf9f7e",
   "metadata": {},
   "source": [
    " Load & Split Your Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e663a7-3811-4a33-a216-61fab567a6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca7b322-f4cd-444d-b25d-6a5e5794845a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and chunk contents of the blog\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb1acff-8273-4097-8dcc-305ae133dbfe",
   "metadata": {},
   "source": [
    "Embed & Store Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99292a59-694b-4ae8-8c75-ed093549d383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index chunks\n",
    "_ = vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3138907f-08c6-4dca-8c3c-48d97d7d9927",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_type = \"similarity\",     # \"mmr\" or \"similarity_score_threshold\" also work\n",
    "    search_kwargs = {\"k\": 4}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71ca238-266a-467b-8299-80641c0d23bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "SYSTEM = \"\"\"You are an expert assistant.\n",
    "Answer *only* from the context between <context></context>;\n",
    "if the answer isn’t there, say “I don't know.”\"\"\"\n",
    "USER = \"\"\"<context>\\n{context}\\n</context>\\n\\nQuestion: {input}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", SYSTEM), (\"user\", USER)])\n",
    "\n",
    "# NEW: wrap llm + prompt in a \"stuff-documents\" chain → Runnable\n",
    "combine_docs_chain = create_stuff_documents_chain(llm, prompt)  \n",
    "\n",
    "# Build the final RAG runnable\n",
    "rag_chain = create_retrieval_chain(retriever, combine_docs_chain) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7410e8-75ad-4982-a009-1e9f68ad021a",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is Task Decomposition?\"\n",
    "result   = rag_chain.invoke({\"input\": question})\n",
    "\n",
    "print(result[\"answer\"])      # grounded answer\n",
    "print(result[\"context\"])     # the stuffed context string (if you need sources, see below)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09dfc3e-d18e-4e9e-8b43-0977c02933ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
