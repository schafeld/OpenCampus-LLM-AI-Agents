{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d892af8-7665-43c4-bce0-bfd835072b69",
   "metadata": {},
   "source": [
    "# First langchain tutorial\n",
    "\n",
    "Langchain is arguably the most famous framework for building first llm applications. This is also why we use it as a start for our journey. If you are more experienced -> don't worry the course will scale up in complexity quite soon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d946d49c-9c87-47bd-b7ab-c08108b2c78c",
   "metadata": {},
   "source": [
    "## Build a simple LLM application with chat models and prompt templates\n",
    "\n",
    "In this first tutorial we'll see how to build a simple LLM application with LangChain. This application will translate text from English into German. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c7866c-6622-4ee3-a6bd-46884acf0252",
   "metadata": {},
   "source": [
    "First up, let's learn how to use a language model by itself. LangChain supports many different language models that you can use interchangeably. For details on getting started with a specific model, refer to [supported integrations](https://python.langchain.com/docs/integrations/chat/).\n",
    "We will stick with ollama for now and our small model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6efdc342-ffda-4122-9206-4d5ea76bec52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -qU \"langchain[ollama]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37c1a254-87ed-475f-856a-5c8dbb24f8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gemma3:1b\", model_provider=\"ollama\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa5c498-e802-4a14-8561-0078ae99f180",
   "metadata": {},
   "source": [
    "Let's first use the model directly. ChatModels are instances of LangChain Runnables, which means they expose a standard interface for interacting with them. To simply call the model, we can pass in a list of messages to the .invoke method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5a4f0ed-dc44-461a-b677-a2dc071c4881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Okay, absolutely! Here are a few ways to translate “Hello Germany!” into Dutch, keeping it friendly and chatty:\\n\\n*   **\"Hoi Deutschland!\"** - This is the most straightforward and common way to say it. It’s friendly and casual.\\n\\n*   **“Hé Duits!”** -  This is a bit more playful and enthusiastic, like you\\'re greeting someone you\\'ve known a while.\\n\\n*   **“Hallo Duits!”** -  A little more formal than “Hoi,” but still warm and welcoming. \\n\\n**Which one do you think sounds best to you?** 😊 \\n\\nDo you want me to translate something else for you?', additional_kwargs={}, response_metadata={'model': 'gemma3:1b', 'created_at': '2025-05-08T08:42:18.499512Z', 'done': True, 'done_reason': 'stop', 'total_duration': 5143043411, 'load_duration': 48794523, 'prompt_eval_count': 37, 'prompt_eval_duration': 217878015, 'eval_count': 144, 'eval_duration': 4875427016, 'model_name': 'gemma3:1b'}, id='run--46908b44-0190-4ce8-9a58-36a2a788f6d1-0', usage_metadata={'input_tokens': 37, 'output_tokens': 144, 'total_tokens': 181})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Translate the following from English into Dutch. Use a firendly, chatty tone of voice.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello Germany!\"},\n",
    "]\n",
    "\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb532d6f-ac0d-43fe-8d8b-75d74c86dadd",
   "metadata": {},
   "source": [
    "Currently we are creating and passing the messages directly to the model. For making this more general we introduce chat templates which are basically those messages containing certain values which we are setting dynamically. Right now it is not important but later for building a real chat it becomes important.\n",
    "\n",
    "Prompt templates are a concept in LangChain designed to assist for this. They take in raw user input and return a prompt that is ready to pass into a language model.\n",
    "\n",
    "Let's create a prompt template here. It will take in two user variables:\n",
    "\n",
    "language: The language to translate text into\n",
    "\n",
    "text: The text to translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "510066d0-219f-4ab0-8035-283f367b2364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = \"Translate the following from English into {language}\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3e6dbdc-9efa-41ea-99f6-49e5351c624e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Translate the following from English into Dutch', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello Germany!', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = prompt_template.invoke({\"language\": \"Dutch\", \"text\": \"Hello Germany!\"})\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6923cd4-a8a1-4805-bb5b-fdf460e17375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most natural translation of \"Hello Germany!\" into Dutch would be:\n",
      "\n",
      "**\"Hallo Duitsland!\"**\n",
      "\n",
      "You could also say:\n",
      "\n",
      "*   **\"Hoi Duitsland!\"** (More informal, like \"Hi Germany!\")\n",
      "\n",
      "Let me know if you'd like other variations!\n"
     ]
    }
   ],
   "source": [
    "response = model.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a58307a-258f-4487-99cf-129002e5245e",
   "metadata": {},
   "source": [
    "**Note:** Langchain templates are just a way to do some basic string and object manipulation, it's something you could also do by hand as we did in the first cell of this notebook. It's just langchains idea how to package that into some format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680af6d6-0b17-4711-8185-c9991029fbb8",
   "metadata": {},
   "source": [
    "## OpenAI\n",
    "\n",
    "Let's now do the same with OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511340bb-3137-49a4-b341-fe1df585437d",
   "metadata": {},
   "source": [
    "Import API key from .env file outside the repository (so we don't risk exposing it inadvertantly to Github and thereby the public)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "161c5cbd-8ffc-48c6-b856-1292366b9a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /Users/user001/anaconda3/lib/python3.11/site-packages (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78884d62-fdaf-4e8f-8316-0391e8842cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(os.path.expanduser(\"~/Projekte/MOOC/OpenCampus/codespace/.env\"))\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  # os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "  raise EnvironmentError(\"OPENAI_API_KEY not found in the .env file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4e84370-3ca1-4c70-978d-1dd367c72cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b94d0667-dc4c-43ae-87a8-8cb1d8b2e957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hallo Duitsland!\n"
     ]
    }
   ],
   "source": [
    "response = model.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cf71f2-11be-4aad-9666-8c5d60123ad6",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Different LLMs do similar things, what differs is how good they are in doing the task. For this simple translation obviously every LLM is able to do it. For more complex tasks you have to see which LLMs is able to do it. We will take a look at that later in the course. Personally I think of different LLMs like different employees -> some are generally smarter, some are better at a certain task and worse at another compared to other employees, at the end it comes down that we test those LLMs on the task at hand and see which ones are able to solve it. Similarly we test humans (exams, assesments centers, ...) if they are able to the task and which one does it best :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8337951d-cd02-45b9-9a71-a48cd67a7e38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
