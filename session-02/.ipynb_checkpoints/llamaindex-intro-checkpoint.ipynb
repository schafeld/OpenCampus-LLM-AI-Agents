{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25c75732-3e10-48c9-89d3-dd43595bbc3d",
   "metadata": {},
   "source": [
    "# First llamaindex tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b5a67c-fc98-44c2-b534-b20025276e51",
   "metadata": {},
   "source": [
    "Prepare to import your API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77edabf7-09fb-4f4c-9930-f75b522ef2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7865bf-2c6d-4fb0-84a9-d6bb4ffd2f5a",
   "metadata": {},
   "source": [
    "Similar to langchain llamaindex provides a lot of functionality to easily work with LLMs. It is one of the most popular frameworks besides langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53499eb2-dacd-4395-ba3b-e413492d1444",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87c39a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your OpenAI API key (required if using OpenAI):  ········\n"
     ]
    }
   ],
   "source": [
    "#skip this section if you have no openai api key\n",
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(os.path.expanduser(\"~/Projekte/MOOC/OpenCampus/codespace/.env\"))\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\n",
    "        prompt=\"Enter your OpenAI API key (required if using OpenAI): \"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee07feb-7ee6-4a6f-8531-7f7770ed6671",
   "metadata": {},
   "source": [
    "Let's ask the model about if it knows about opencampus.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7100bf1a-c1bc-4928-8bba-8e71f580aac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of my last knowledge update in October 2023, \"opencampus.sh\" does not refer to a widely recognized or established platform, tool, or concept. It could potentially be a specific project, website, or script related to open-source education, campus management, or similar themes, but without more context, it's difficult to provide a precise answer.\n",
      "\n",
      "If \"opencampus.sh\" is a new initiative, project, or tool that has emerged after my last update, I recommend checking the latest online resources, such as GitHub, educational technology forums, or official websites, for the most accurate and up-to-date information. If you have more context or details about what \"opencampus.sh\" refers to, I would be happy to help further!\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "response = OpenAI(model=\"gpt-4o-mini\").complete(\"What is opencampus.sh \")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6c12ad-393a-4f24-802a-a2da23f9b900",
   "metadata": {},
   "source": [
    "Uhh that is kind of a bummer, it does not even know about us. Probably because opencampus.sh is only fewly mentioned on any website etc.\n",
    "\n",
    "Let's try out the new 4.1 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76759e34-856d-4e8e-8a1e-2c9b7f65f91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "llm = OpenAI(model=\"gpt-4.1-mini\")\n",
    "messages = [\n",
    "    ChatMessage(role=\"system\", content=\"You are a helpful assistant.\"),\n",
    "    ChatMessage(role=\"user\", content=\"What is opencampus.sh\"),\n",
    "]\n",
    "chat_response = llm.chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3281939d-6c07-46f0-bd01-b647e8759e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: Opencampus.sh is a platform designed to provide online courses, training, and educational resources. It typically offers a variety of learning materials, including video lectures, tutorials, and interactive content, aimed at helping users acquire new skills or knowledge in different fields. The platform may focus on areas such as technology, programming, business, or other professional development topics.\n",
      "\n",
      "If you have a specific context or need more detailed information about opencampus.sh, please let me know!\n"
     ]
    }
   ],
   "source": [
    "print(chat_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b1b618-d33f-45fc-8f6e-70283488fbb0",
   "metadata": {},
   "source": [
    "Hmm seems to go a bit into hallucinations here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681ec06c-f4df-4894-8c06-69527c03494c",
   "metadata": {},
   "source": [
    "## Olama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0033ab4b-9ff1-43d9-ad66-32651fd65c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install llama-index-llms-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c2e6417-107c-449e-8cf1-9db4a981348f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: Okay, let's break down what OpenCompute Accelerated Shared Memory (OCASSH) is. It’s a fascinating and increasingly important technology, especially for high-performance computing (HPC) and scientific computing. Here's a breakdown of what it is, why it's valuable, and how it works:\n",
      "\n",
      "**What is OpenCompute Accelerated Shared Memory (OCASSH)?**\n",
      "\n",
      "OCASSH is a **hardware-accelerated shared memory system** developed by NVIDIA.  It’s designed to significantly speed up certain types of computations, particularly those involving large matrices and tensors, which are common in fields like:\n",
      "\n",
      "*   **Deep Learning:** Training and inference of deep neural networks.\n",
      "*   **Scientific Simulations:**  Simulations in areas like fluid dynamics, climate modeling, and materials science.\n",
      "*   **Data Science:**  Analyzing and processing large datasets.\n",
      "\n",
      "**Here's a more detailed explanation of its key aspects:**\n",
      "\n",
      "1.  **Shared Memory:**  The core concept is *shared memory*.  Instead of each processor having its own dedicated memory space, OCASSH creates a large, shared pool of memory that multiple processors can access.\n",
      "\n",
      "2.  **GPU-Optimized:**  OCASSH is specifically designed to work *very* efficiently with NVIDIA GPUs. It leverages the GPU's architecture to achieve substantial performance gains.  It’s not just a general-purpose shared memory system; it’s tailored for the unique characteristics of GPU computations.\n",
      "\n",
      "3.  **Matrix Multiplication Optimization:**  A significant portion of OCASSH's benefits comes from its ability to dramatically accelerate matrix multiplication – the backbone of deep learning and many scientific simulations.  It achieves this through a sophisticated technique called *Matrix-Vector Multiplication (MV) Acceleration*.  It efficiently transforms the matrix multiplication operation into a series of vector operations.\n",
      "\n",
      "4.  **How it Works (Simplified):**\n",
      "    *   **Data Partitioning:**  OCASSH divides data into smaller chunks.\n",
      "    *   **Vectorization:** It uses vector instructions to process these chunks.\n",
      "    *   **Parallelism:** It then utilizes the GPU’s massively parallel architecture to process the data concurrently across multiple cores.\n",
      "\n",
      "**Why is it Important?**\n",
      "\n",
      "*   **Speedup:**  OCASSH can often achieve speeds that are *significantly* faster than traditional shared memory solutions, particularly for large matrix operations.\n",
      "*   **Reduced Memory Bottlenecks:**  By accelerating matrix multiplication, it can alleviate some of the memory bottlenecks that can slow down computations.\n",
      "*   **Scalability:**  It's designed to scale well across multiple GPUs, making it suitable for large-scale HPC clusters.\n",
      "\n",
      "**Key Benefits in Specific Applications:**\n",
      "\n",
      "*   **Deep Learning Training:**  Accelerating the forward and backward passes in deep learning models.\n",
      "*   **Image Processing:**  Speeding up tasks like image filtering and segmentation.\n",
      "*   **Scientific Modeling:**  Solving complex equations in fluid dynamics, climate modeling, and other fields.\n",
      "\n",
      "\n",
      "**Resources for More Information:**\n",
      "\n",
      "*   **NVIDIA OCASSH Documentation:** [https://developer.nvidia.com/ocassh](https://developer.nvidia.com/ocassh) - This is the official source of information.\n",
      "*   **NVIDIA Blog:** [https://developer.nvidia.com/blog/ocassh/](https://developer.nvidia.com/blog/ocassh/) - They frequently post updates and insights.\n",
      "\n",
      "---\n",
      "\n",
      "**To help me tailor my response even further, could you tell me:**\n",
      "\n",
      "*   Are you interested in a deeper dive into a specific use case (e.g., deep learning)?\n",
      "*   Are you wondering about the technical details of how it works (e.g., the MV Acceleration technique)?\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "llm = Ollama(model=\"gemma3:1b\", request_timeout=60.0)\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(role=\"system\", content=\"You are a helpful assistant.\"),\n",
    "    ChatMessage(role=\"user\", content=\"What is opencampus.sh\"),\n",
    "]\n",
    "\n",
    "chat_response = llm.chat(messages)\n",
    "print(chat_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0149a62e-d2ca-4d70-82cc-586e7c1baafe",
   "metadata": {},
   "source": [
    "Oh dear.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1a71c5-25ce-4d7a-8bc8-82a2c65bd465",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We have libraries which wrap the llm call into functions and classes of their liking. We gain that we can easily switch between different model providers but loose at lot of flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec892f8-1dd8-44b4-a470-008e0922435c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
