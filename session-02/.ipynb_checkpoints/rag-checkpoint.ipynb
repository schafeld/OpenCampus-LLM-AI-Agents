{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6505298-5791-4f55-b6fc-4481ce2717ba",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c723f2c6-0077-4455-a940-55dcc9dcb745",
   "metadata": {},
   "source": [
    "We can think of large language models (LLMs) as people who have read every book in a library up until a certain date. 📚 They've remembered the content with varying degrees of accuracy, but their knowledge base consists of everything that was in that library.\n",
    "\n",
    "In the case of LLMs, the \"library\" is actually a massive collection of internet documents—heavily filtered for quality—along with some curated private datasets, such as newspaper archives and academic papers.\n",
    "\n",
    "If we need the model to understand or use information that was not (or only barely) included in its training data, we have to provide that information in the prompt. However, there's a limit to how much information we can give at once. This is due to something called a context window—a maximum number of tokens (e.g., 8,096 tokens) that the model can process in a single prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb34eb5-4007-4b59-bc71-c6e975ba6aee",
   "metadata": {},
   "source": [
    "## Augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2aa41425-fe74-457f-adfb-3beb69b8d208",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, let's brainstorm some Mother's Day gifts in the 20-50 Euro range! Here’s a breakdown of suggestions, categorized by interest, to help you find the perfect one:\n",
       "\n",
       "**1. Cozy & Relaxing (Around 20-30 Euros):**\n",
       "\n",
       "* **Luxury Bath Bomb or Bubble Bath Set:** A beautifully scented bath bomb or a set of lovely bubble bath is always appreciated. (Think Lavender, Citrus, or Rose scents).\n",
       "* **Scented Candle:** A small, nicely scented candle (soy or beeswax candles are a good choice) is a classic and comforting gift.\n",
       "* **Cozy Socks & Blanket:** A pair of plush, warm socks with a pretty design or a small, soft blanket.\n",
       "\n",
       "**2. Food & Drink Focused (Around 20-40 Euros):**\n",
       "\n",
       "* **Gourmet Tea or Coffee Gift Set:** A selection of premium teas or coffee beans and a nice mug makes a great treat.\n",
       "* **Small Jar of Gourmet Jam or Honey:**  A delightful spread for toast or scones.\n",
       "* **Chocolate Box:** A beautifully packaged box of artisanal chocolates - a splurge, but a thoughtful choice.\n",
       "* **Specialty Biscotti:**  A box of delicious biscotti is a comforting and sophisticated treat.\n",
       "\n",
       "\n",
       "**3.  Personalized & Thoughtful (Around 30-45 Euros):**\n",
       "\n",
       "* **Photo Calendar:** A calendar featuring family photos – a lovely and sentimental gift. (You can find really affordable ones!)\n",
       "* **Personalized Mug:**  A mug with a photo, a short message, or a cute design.\n",
       "* **Small Succulent or Plant:**  A little greenery adds a touch of life to any space. (Consider a low-maintenance variety.)\n",
       "* **Customized Stationery:**  If she loves to write, a set of personalized notecards or a notebook with her name is sweet.\n",
       "\n",
       "\n",
       "**4.  Experiences (Worth considering if you have a budget):**\n",
       "\n",
       "* **Coffee or Tea Tasting:** A voucher for a local coffee or tea shop to enjoy.\n",
       "* **Small Spa Treatment Gift Voucher:**  A voucher for a manicure/pedicure or a facial. (Could be a little higher, but still within the range.)\n",
       "\n",
       "\n",
       "**To help me narrow it down further and give you more specific recommendations, could you tell me:**\n",
       "\n",
       "*   **What are her interests?** (e.g., gardening, cooking, reading, fashion, travel, etc.)\n",
       "*   **What's her personality like?** (e.g., practical, sentimental, luxurious, humorous?)\n",
       "*   **What’s your preferred style of gift?** (e.g., small and thoughtful, bigger and more extravagant?)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ollama import chat\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "chat_response = chat(model='gemma3:1b', messages=[\n",
    "    {\n",
    "    'role': 'user',\n",
    "    'content': \"Hey, I am looking for a mother's day gift, please suggest something in a price range from 20 to 50 Euros.\",\n",
    "    },\n",
    "])\n",
    "\n",
    "display(Markdown(chat_response.message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf2a728-5bc2-4566-b7e8-62773f492601",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80012c9-7659-4a04-a74e-d56188a78a0e",
   "metadata": {},
   "source": [
    "~~Let's now do the same with the context that it is currently Easter and see how the model adapts its gift recommendations.~~ \n",
    "\n",
    "Let's be a little more generous and say 40 to 80 euros and no flowers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9340b481-945b-46f4-a7d0-d647fa69a53e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, let's brainstorm some Mother’s Day gifts for around €40-€80! Here's a breakdown of ideas, categorized by vibe, with estimated price ranges:\n",
       "\n",
       "**1. Practical & Luxurious (Around €40-€60)**\n",
       "\n",
       "* **High-Quality Tea & Gourmet Treats Basket:** A beautifully packaged selection of artisan teas (like Earl Grey, herbal blends), a nice box of chocolates or macarons, and a small, luxurious item like a scented candle or a lovely hand cream. (€40-€60)\n",
       "* **Cozy Throw Blanket:** A super soft and stylish throw blanket is always appreciated. Look for something in a neutral color or with a subtle pattern. (€40-€60)\n",
       "* **Portable Espresso Maker:** If your mom enjoys coffee, a small, portable espresso maker is a fantastic and practical gift. (€40-€60)\n",
       "* **Leather Keychain with a Meaningful Initials:** A beautifully crafted leather keychain with her initials or a short, meaningful message is a thoughtful and durable gift. (€20-€40)\n",
       "\n",
       "\n",
       "**2. Relaxation & Self-Care (Around €40-€70)**\n",
       "\n",
       "* **Bath Bombs & Essential Oil Set:** A luxurious set of bath bombs with calming scents, along with a few essential oils (lavender, eucalyptus, etc.) – creates a spa-like experience. (€25-€45)\n",
       "* **Massage Oil & a Luxurious Body Lotion:** A nice massage oil (particularly a moisturizing one) combined with a high-quality body lotion is a great way to encourage relaxation. (€30-€50)\n",
       "* **Luxurious Face Mask Set:** A collection of different face masks caters to different skin types, offering a pampering experience. (€30-€50) \n",
       "* **Adult Coloring Book & Nice Colored Pencils:**  A relaxing activity can be wonderful, especially with a beautiful coloring book and some quality colored pencils. (€20-€40)\n",
       "\n",
       "\n",
       "**3. Hobby-Related (Around €40-€70)**\n",
       "\n",
       "* **Knitting/Crochet Supplies:** If she enjoys crafting, a new stitch, a small yarn, or a helpful pattern can be a welcome gift. (€20-€40)\n",
       "* **Cooking/Baking Kit:** If she loves to cook, a mini kit with some new spices, or a unique baking tool could be very appreciated. (€30-€50) \n",
       "* **Puzzle Book (Her Favorite Genre):**  A challenging puzzle book related to her interests – gardening, history, or mysteries – can provide hours of entertainment. (€20-€40)\n",
       "\n",
       "\n",
       "**4.  Personalized/Sentimental (Around €40-€80)**\n",
       "\n",
       "* **Custom Photo Album or Calendar:** Fill it with family photos - this is a heartfelt and lasting gift. (€30-€60)\n",
       "* **Personalized Mug with a Photo:**  A mug featuring a cherished photo creates a constant reminder of loved ones. (€20-€40)\n",
       "* **Handwritten Letter:** Sometimes, a heartfelt, handwritten letter expressing your appreciation is the most meaningful gift. (€0 - €20, depending on how elaborate you want it)\n",
       "\n",
       "\n",
       "\n",
       "**To help me narrow it down even further and give you even *better* recommendations, could you tell me:**\n",
       "\n",
       "*   **What are her main interests/hobbies?** (e.g., gardening, reading, cooking, crafting, travel, etc.)\n",
       "*   **What's her personal style like?** (e.g., minimalist, cozy, colorful, classic, bohemian?)\n",
       "*   **What kind of gifts does she *usually* appreciate?** (e.g., thoughtful, practical, fun, luxurious?)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "extra_content = \"Set a price range of 40 to 80 Euros. Do not chose flowers as gift.\"\n",
    "\n",
    "chat_response = chat(model='gemma3:1b', messages=[\n",
    "    {\n",
    "    'role': 'user',\n",
    "    'content': \"Hey, I am looking for some mother's day gifts, please recommend some.\" + extra_content,\n",
    "    },\n",
    "])\n",
    "\n",
    "display(Markdown(chat_response.message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14775cd0-e655-4470-a6fa-c221f28caa09",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438bd64b-0c52-49a8-8004-f5db1a3c9a6b",
   "metadata": {},
   "source": [
    "The notebook is named **RAG (Retrieval-Augmented Generation)**. In this context, \"augmentation\" refers to enriching the generation process with additional, task-specific information. However, when the volume of available data is too large to insert in full, we must **retrieve** only the most relevant parts to use as context.\n",
    "\n",
    "**Note**: Having a context window of for example 128.000 tokens does not mean that we should fill it up until that. Adding more and more tokens leads to more latency (i.e. it takes longer for the model to serve the user) and there could be also an information overload i.e. that the information fits into the context window but there is just too much information for the model may overlook important details. (This in the end could be only found out by testing, but personally I think in terms of information density and complexity. The german \"Bild Zeitung\" has for example a very low information density and complexity while a medical textbook has a very high information density and complexity. Hence doing Q&A over 30.000 tokens of Bild articles is definitively not the same as doing Q&A over 30.000 tokens of medical textbook pages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0222e9de-f4d9-4f41-98ca-32d1f5d283f3",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03b4238-d04f-4405-b234-f1c45626407c",
   "metadata": {},
   "source": [
    "For actually retrieving something we need data. Let us consider for now this simplified database of ten different entries/documents about talks given on a future festival. We want to build a small RAG chatbot for festival visitors where they can ask for their specific interests what talks would fits the most. \n",
    "\n",
    "**Note** The dataset is so small that I would personally just completly put it always in the context but for demonstration purposes let's use the talks as single documents. Read through them to get a grasp of the data below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fe3fb4-a28e-4553-bb4a-3d46c6311bb7",
   "metadata": {},
   "source": [
    "##### 1. 🌿 **The Next Nature: Designing with the Future in Mind**  \n",
    "*Explore how biomimicry, regenerative design, and synthetic biology are reshaping architecture, materials, and cities. Where does design end and nature begin?*\n",
    "\n",
    "---\n",
    "\n",
    "##### 2. 🧠 **Neural Frontiers: The Brain-Computer Interface Revolution**  \n",
    "*From thought-controlled devices to memory enhancement, dive into the fast-evolving world of brain tech — and the ethical mazes it brings.*\n",
    "\n",
    "---\n",
    "\n",
    "##### 3. 🎨🤖 **AI & the Soul of Art: Who Really Owns Creativity?**  \n",
    "*Artists, coders, and philosophers debate the rise of generative art. Can machines make meaning? And where does human intuition still reign supreme?*\n",
    "\n",
    "---\n",
    "\n",
    "##### 4. 💼🚫 **Post-Work: Imagining a Life Beyond Jobs**  \n",
    "*As automation reshapes labor, what comes next? UBI, digital nomadism, reputation economies — a candid discussion about freedom, purpose, and survival.*\n",
    "\n",
    "---\n",
    "\n",
    "##### 5. 🪐 **Planet B: Terraforming Ideas for Earth 2.0**  \n",
    "*Science fiction meets climate urgency. This talk blends real research with wild speculation — from Mars domes to floating cities in the clouds of Venus.*\n",
    "\n",
    "---\n",
    "\n",
    "##### 6. 🕶️🌐 **Reality is Optional: The Rise of Immersive Worlds**  \n",
    "*VR, AR, XR — and whatever’s next. What happens when our digital spaces feel more real than reality itself? And who gets to write the rules?*\n",
    "\n",
    "---\n",
    "\n",
    "##### 7. 🧬⏳ **The Ethics of Immortality: Living Forever in a Mortal World**  \n",
    "*Cryonics, gene editing, mind uploading — tech is chasing eternal life. But what would it mean for love, loss, and the human condition?*\n",
    "\n",
    "---\n",
    "\n",
    "##### 8. 💻⚖️ **Code as Culture: Programming the Future We Want**  \n",
    "*Code is not neutral — it shapes societies. This talk explores how developers are becoming the new lawmakers, and how we hold them accountable.*\n",
    "\n",
    "---\n",
    "\n",
    "##### 9. 🕸️🛠️ **The Wild Web: Reclaiming the Internet from Algorithms**  \n",
    "*Can we rebuild the web for people, not profit? Meet the rebels, hackers, and dreamers creating decentralized, community-first digital spaces.*\n",
    "\n",
    "---\n",
    "\n",
    "##### 10. 🕰️🚀 **Time Travelers Welcome: Building the Long Now**  \n",
    "*Futurists, historians, and deep-time thinkers gather to explore projects that think in centuries — from 10,000-year clocks to interstellar archives.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70a321a-06c9-46e7-ab5d-b4dc55744a24",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc750419-b34a-41fd-9790-e10e16b0bdbf",
   "metadata": {},
   "source": [
    "Okay, we see depending on your interests there might different talks to be the most interesting ones.\n",
    "\n",
    "First we need to prepare the talks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8da75bb0-a6c0-407e-ae2f-3f53211e4738",
   "metadata": {},
   "outputs": [],
   "source": [
    "festival_talks = [\n",
    "    \"🌿 The Next Nature: Designing with the Future in Mind — Explore how biomimicry, regenerative design, and synthetic biology are reshaping architecture, materials, and cities. Where does design end and nature begin?\",\n",
    "    \"🧠 Neural Frontiers: The Brain-Computer Interface Revolution — From thought-controlled devices to memory enhancement, dive into the fast-evolving world of brain tech — and the ethical mazes it brings.\",\n",
    "    \"🎨🤖 AI & the Soul of Art: Who Really Owns Creativity? — Artists, coders, and philosophers debate the rise of generative art. Can machines make meaning? And where does human intuition still reign supreme?\",\n",
    "    \"💼🚫 Post-Work: Imagining a Life Beyond Jobs — As automation reshapes labor, what comes next? UBI, digital nomadism, reputation economies — a candid discussion about freedom, purpose, and survival.\",\n",
    "    \"🪐 Planet B: Terraforming Ideas for Earth 2.0 — Science fiction meets climate urgency. This talk blends real research with wild speculation — from Mars domes to floating cities in the clouds of Venus.\",\n",
    "    \"🕶️🌐 Reality is Optional: The Rise of Immersive Worlds — VR, AR, XR — and whatever’s next. What happens when our digital spaces feel more real than reality itself? And who gets to write the rules?\",\n",
    "    \"🧬⏳ The Ethics of Immortality: Living Forever in a Mortal World — Cryonics, gene editing, mind uploading — tech is chasing eternal life. But what would it mean for love, loss, and the human condition?\",\n",
    "    \"💻⚖️ Code as Culture: Programming the Future We Want — Code is not neutral — it shapes societies. This talk explores how developers are becoming the new lawmakers, and how we hold them accountable.\",\n",
    "    \"🕸️🛠️ The Wild Web: Reclaiming the Internet from Algorithms — Can we rebuild the web for people, not profit? Meet the rebels, hackers, and dreamers creating decentralized, community-first digital spaces.\",\n",
    "    \"🕰️🚀 Time Travelers Welcome: Building the Long Now — Futurists, historians, and deep-time thinkers gather to explore projects that think in centuries — from 10,000-year clocks to interstellar archives.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a773e3eb-4ece-4508-a2da-e716960d0a59",
   "metadata": {},
   "source": [
    "For the retrieval part we need a kind of search based on the question over talks. The standard way was to do a key word search but with the upcoming AI era that default changed to a vector search. Therefore we need another type of model: An embedding model. An embedding model takes a text and transforms it into a vector of a certain size (e.g. 1*1048). Every text is mapped to a the same vector size. The embedding models are pretrained that they map similar texts close to each other in the vector space (so LLMs aren't the only cool models). Luckily ollama and also all major providers like openai and huggingface also provide embedding models. We will now take the currently best embedding models from ollama (lukily the embedding models are usually really small < 1GB)\n",
    "\n",
    "Run **ollama pull nomic-embed-text** in your terminal\n",
    "\n",
    "Ollama is able to serve the LLM and the embedding model at the same time (given enough (V)RAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c30687eb-a947-4088-912b-c7e18834a1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.041261844, 0.06610261, -0.17822084, -0.016629975, 0.021285325, -0.026365353, -0.011967031, 0.017026164, -0.0102254655, -0.031980272, 0.050730683, -0.019034838, 0.045592844, -0.008341599, 0.012121793, 0.020955326, -0.040141687, -0.062111855, -0.030746346, -0.0075697703, -0.016780589, -0.02938528, -0.0114756655, 0.03243297, 0.089692056, 0.053597085, -0.027188728, 0.019495934, -0.0077963886, 0.058877368, 0.07543706, -0.028498396, 0.021128139, 0.010190087, 0.04324336, 0.033847433, -0.005320718, 0.011530684, -0.005175544, 0.001702866, 0.041846916, 0.009595384, 0.031900715, -0.005481884, 0.048807073, -0.07657718, 0.020511717, 0.04466181, 0.050690647, -0.071095616, -0.020409374, -0.028782988, -0.02661789, 0.008561729, 0.06537089, -0.019210912, -0.018011179, -0.011410761, 0.007950822, 0.016235167, 0.08522264, 0.030174375, 0.0014069363, 0.06269761, 0.019721037, 0.05231696, -0.012256007, 0.07123785, -0.008217562, -0.012078117, 0.057234813, 0.0058020097, -0.0028707415, 0.012909217, -0.019607263, 0.019967904, -0.0016380567, -0.07573497, -0.010009757, 0.0013075554, 0.012321612, -0.027521914, 0.08478139, -0.054996077, 0.058566175, -0.027072622, 0.023049517, -0.023820482, -0.005300369, 0.07861292, -0.010064279, 0.03549347, -0.018191244, 0.03466201, -0.03469749, -0.0010808783, 0.015417769, 0.008149676, -0.09677557, 0.030495428, -0.05065322, -0.02723895, -0.050120674, 0.004603859, 0.07357937, -0.0062409826, -0.024723936, -0.027263898, -0.014991426, 0.018931258, 0.0107156625, 0.023673763, -0.007861183, -0.037506875, 0.009566431, 0.018956183, 0.0771353, -0.07042636, 0.074713886, -0.008246013, -0.0049894787, -0.0005541649, 0.008600693, -0.03336173, 0.0026175706, -0.005629038, -0.04046585, -0.018442692, -0.03560128, -0.058512215, -0.013599383, 0.00012284635, -0.03865538, -0.004990897, 0.028741844, 0.02290092, -0.028082423, -0.0580046, -0.0014023542, 0.019626139, 0.012107482, -0.015907243, -0.003898641, 0.024070127, 0.0067334925, -0.03988605, 0.043363404, 0.014259297, 0.0045425706, -0.009948921, 0.010965083, 0.007185091, -0.0108531825, 0.05773988, -0.01031031, -0.043188773, -0.03578709, -0.046831593, 0.06312791, 0.021101985, 0.016342457, 0.025517913, -0.070362225, 0.028639141, 0.034620665, -0.060252536, 0.046751972, 0.04919943, 0.055420093, 0.006544652, -0.102746494, -0.060121737, 0.006061256, 0.0027825956, 0.001422752, -0.07002615, -0.0059385807, -0.020958802, 0.030442059, 0.017636914, 0.06228764, -0.03182462, -0.002198461, 0.03143137, -0.041040022, 0.007099002, -0.031667363, 0.006667934, -0.019116981, -0.01735033, 0.027283913, -0.014189747, -0.07523076, -0.0031575218, -0.05731284, -0.0135078775, 0.029597634, 0.010821562, 0.0610727, -0.046688806, -0.032525226, -0.007862847, -0.033057332, 0.047309004, 0.023079885, 0.053215124, -0.019016184, 0.03400619, -0.0007650388, 0.012124145, 0.061494686, 0.01907931, 0.01847696, 0.0149482, 0.01976902, 0.022306181, -0.02146505, -0.059321914, -0.006434348, 0.016191572, 0.06674091, 0.010531407, 0.0340692, -0.0067312233, 0.045229424, -0.0972486, -0.04025655, 0.042513262, -0.001463023, 0.050947793, -0.021285221, -0.034180254, 0.014506826, 0.025294408, -0.0014406003, 0.015944704, -0.011284291, 0.057604965, -0.0020183206, -0.025971256, -0.012363204, 0.003241517, -0.032522418, -0.053486705, -0.06449439, 0.00034775963, -0.06105988, -0.035042126, -0.03404379, -0.026336577, 0.03153584, 0.051842608, 0.001730481, -0.049985774, 0.021414153, 0.015178761, 0.0045388225, -0.018323736, 0.016991494, -0.023007477, 0.06840927, -0.04360892, 0.027757252, -0.04666012, 0.038785085, 0.03329435, -0.046078652, -0.011568836, 0.0022721519, -0.06349771, -0.017113885, 0.0521814, -0.004434107, 0.01845646, 0.0023687554, 0.0017266518, 0.004268205, 0.029473761, -0.047988847, 0.03303551, -0.05937103, 0.010909287, -0.037071705, 0.016695708, 0.025928445, 0.022505775, 0.0081921285, 0.013390862, 0.05149651, 0.0039771674, 0.06605291, 0.043802608, 0.017410608, 0.093693316, -0.05733408, -0.010311986, 0.005979453, -0.00018053266, -0.049208935, 0.008144894, 0.03854903, 0.0075402944, 0.03503729, 0.025466446, -0.0029246763, -0.04544849, 0.044968016, 0.02792069, 0.022315657, -0.008698359, -0.029203793, 0.00079169887, -0.04309488, -0.044906884, -0.047102455, -0.010952327, -0.005069556, -0.007092198, 0.040735897, -0.03569088, -0.04369955, 0.006410663, -0.06610535, -0.04885942, -0.01155692, 0.016716018, -0.01640913, 0.011100249, 0.01821736, -0.033821393, -0.010914415, 0.059748076, 0.030981809, -0.01693423, 0.029513642, 0.010131294, 0.031238405, -0.016705377, -0.009822667, 0.0148596065, 0.010608966, 0.003223309, -0.013639376, -0.031132622, -0.042236738, 0.0075639165, 0.036098775, -0.06473671, 0.025508234, 0.008147318, -0.0393202, 0.0076937806, -0.04731716, 0.02824298, -0.026731372, -0.015583864, -0.012156988, -0.010211574, 0.03870888, 0.005452084, 0.0039097546, 0.02022994, -0.024979372, -0.026734648, -0.015597383, -0.018316817, 0.060190294, -0.0129882265, 0.0035591107, 0.013449742, 0.000613023, 0.005081522, 0.057665363, 0.025932223, 0.00893779, 0.008103334, -0.095361955, 0.016978651, 0.00063327467, -0.016586995, 0.009763932, -0.02113581, 0.008080507, -0.012375988, -0.016121292, 0.015275948, 0.018273098, -0.03578146, -0.018831598, 0.037845477, -0.054880887, -0.013435556, -0.0651318, 0.0029712915, -0.026175547, 0.005840606, 0.050923437, 0.014115034, -0.0033610042, 0.045487907, -0.027757438, -0.05790083, -0.04305193, -0.094478965, -0.01640434, 0.0030560335, -0.033670053, 0.0081733335, 0.059613433, 0.03801225, 0.061929066, 0.048754673, 0.056431506, -0.023102304, 0.0440874, 0.008188146, 0.04646749, 0.012021486, 0.022967463, -0.032482002, 0.02781134, -0.017368909, 0.038276676, -0.014903475, 0.015129761, 0.02248403, 0.049328506, 0.042012624, -0.034931853, -0.036511023, -0.0105548715, 0.01332724, 0.018801125, -0.0424254, -0.03558112, 0.010838322, 0.032946173, 0.022925083, -0.0048740003, -0.0032654537, 0.031506963, 0.017165015, 0.0055603916, -0.016921284, 0.0058197347, 0.07319338, 0.07790029, 0.0005359749, -0.019253533, -0.05023686, 0.052424457, 0.023415763, 0.039063543, 0.03943625, 0.098393075, -0.0022965465, -0.029028205, 0.042663015, 0.07937024, 0.026906542, 0.013156038, -0.028775888, -0.03699865, 0.03903422, 0.034073424, 0.00590467, -0.031776026, -0.0652945, 0.032039918, 0.045245025, 0.04109298, 0.03425065, 0.023593765, -0.025192967, -0.06544108, 0.018432127, -0.014082096, 0.013189878, 0.0044598687, 0.04369332, 0.004458321, -0.030145714, -0.031394962, -0.062126357, -0.012004765, 0.033186525, 0.047858104, -0.0061903354, -0.013389626, -0.029437961, 0.035478614, 0.0029788713, -0.007901012, -0.0075593796, 0.05908586, -0.026713202, -0.0018830962, -0.03536017, 0.039797362, -0.0765618, 0.016054215, 0.033838827, 0.0024374642, 0.008432586, -0.024143914, 0.05168044, 0.034384876, -0.06653125, -0.020510627, 0.035282746, -0.023669748, -0.008736064, -0.05862953, 0.03835192, -0.0030390788, -0.018751608, 0.029520547, 0.039814778, -0.048235144, 0.034923293, 0.0031583698, -0.038024753, -0.0028421183, -0.021391965, -0.07681284, 0.009208831, 0.008557135, -0.01805024, 0.041055884, 0.01017189, 0.0031555716, -0.025864389, -0.067671366, -0.017058738, -0.036554337, 0.0047433153, -0.05805243, 0.039903633, 0.09526261, -0.009431802, -0.009203346, -0.029769579, 0.01847272, -0.04118087, -0.015431246, 0.00028522417, -0.06496267, 0.015146573, -0.03580718, -0.026517633, 0.0032417953, -0.06779133, 0.01779508, -0.015288993, 0.009396021, -0.0065537244, -0.023512958, 0.036884688, -0.052971058, 0.046575688, 0.024007322, -0.014158702, 0.08579194, 0.0038225504, 0.013964859, -0.00013115566, -0.003205167, -0.025511025, -0.009424716, -0.02316152, 0.021231694, -0.05228084, 0.031647526, 0.041276705, 0.0512816, -0.047350608, 0.019961998, 0.00662218, -0.015920475, -0.02089411, 0.049527798, -0.0554296, 0.057460763, 0.033767536, -0.009686168, 0.01193903, -0.016574923, 0.011311557, 0.031533837, -0.050715555, -0.0022788723, 0.01209327, -0.007154828, -0.05554746, -0.008046286, -0.004521044, -0.053551048, -0.0556821, -0.025163053, -0.06332743, -0.030288262, 0.006400328, -0.00037465396, -0.006366821, -0.012968688, 0.07302117, 0.0079716975, -0.024552751, 0.02164737, -0.011005176, 0.005020371, -0.011613615, -0.029870005, -0.032520976, -0.008252962, -0.016301785, 0.050627902, -0.03290413, -0.014568051, -0.024217054, -0.012767683, -0.034645915, 0.031457752, 0.026307328, 0.04170528, -0.0017627291, -0.037705395, -0.053451657, -0.0024208042, 0.04067085, 0.014845743, 0.041104965, -0.054030005, -0.036328804, -0.058561113, 0.011447365, -0.024183538, -0.030401904, -0.0058469456, 0.002441355, 0.03812954, 0.0131966425, -0.05193179, 0.049889416, 0.08836934, -0.044128127, 0.061849978, 0.02489549, 0.036444765, -0.013619514, 0.013959242, 0.037801076, 0.082032956, -0.03658111, 0.015196228, -0.017897192, 0.036013115, 0.013013129, -0.019688014, -0.037726678, 0.006756225, -0.0033688538, -0.0018316524, 0.039419297, 0.0012047087, -0.0271211, -0.031839862, 0.06480085, -0.10145659, -0.010386156, 0.058282286, 0.041339416, -0.037980083, -0.053501602, 0.03360561, 0.025267657, 0.026065627, 0.05394999, 0.0032775986, -0.041077804, -0.041256815, -0.019722585, 0.034448188, 0.008936489, 0.035932966, -0.0063322177, -0.00392455, -0.030350318, 0.015138273, -0.07737598, -0.017247263, -0.05231326, -0.008320482, -0.014869139, -0.012805097, -0.0053767837, -0.047704272, -0.028814059, -0.006590697, 0.015973806, -0.048141055, 0.013136488, 0.00349936, 0.057954334, 0.013526824, 0.0020817495, 0.01606724, 0.03157244, -0.011575858, -0.026402684, -0.02360557, 0.021939233, -0.023491539, 0.013757956, 0.0530376, 0.0013611539, -0.05185761, 0.009138485, 0.011451264, 0.032089822, 0.050954938, -0.019548425, -0.07469466, 0.008924565, 0.0013364187, -0.028877866, 0.021982929, -0.029913088, -0.017342933, -0.009530373, 0.05239216, -0.015489805, -0.052121192, 0.039186172, 0.0068074986, 0.022692977, -0.009391406, -0.009407618, -0.009185019, 0.019615823, -0.05581229, -0.004883604, -0.018839998, 0.04147337, 0.0032962463, -0.013893729, 0.005441632, -0.006628409, 0.0042835088, -0.032750748, -0.0090158675, -0.03340896, -0.028839247, -0.039623935, -0.005334875, 0.03591129, 0.005308739, 0.032345694, 0.116614036, 0.022201305, -0.00550563, -0.027946448, 0.039631575, 0.057572924, -0.03996958, -0.02060164, -0.017392367, 0.03125003]]\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "from ollama import embed\n",
    "\n",
    "# Generate an embedding for a single input\n",
    "response = embed(model='nomic-embed-text', input='opencampus is the best!')\n",
    "\n",
    "# Access the embedding vector\n",
    "embedding = response['embeddings']\n",
    "print(embedding)\n",
    "print(len(embedding[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1951f143-cc04-4781-96b4-3a2827e2cf99",
   "metadata": {},
   "source": [
    "As we can see the text is converted into a vector with a size 1*768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c55a518-e263-4b1e-ba4b-f91279c83a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an embedding for a batch input\n",
    "response = embed(model='nomic-embed-text', input=festival_talks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21c50bd5-d833-4814-a5e5-41a3b56b8594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 768 768\n"
     ]
    }
   ],
   "source": [
    "# Access the embedding vectors\n",
    "embedding = response['embeddings']\n",
    "\n",
    "print(len(embedding), len(embedding[0]), len(embedding[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2692403d-3899-4cab-927d-8f696e968550",
   "metadata": {},
   "source": [
    "So we can see now that we have a list of 10 entries each with a vector with 768 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c028c42-0a54-48d1-bcb6-dc47a95b7655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/user001/anaconda3/lib/python3.11/site-packages (1.24.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e08afec3-7916-40a1-8fa3-d3d47321f2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest vector index: 2\n",
      "Distance: 0.8336664842668186\n",
      "Sorted indices by distance: [2 1 4 3 7 5 9 8 0 6]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vectors = np.array(embedding)\n",
    "\n",
    "query = np.array(embed(model='nomic-embed-text', input='I am interested in AI talks')['embeddings'])\n",
    "\n",
    "# Compute Euclidean distances\n",
    "differences = vectors - query  # shape: (n_vectors, vector_dim)\n",
    "distances = np.linalg.norm(differences, axis=1)\n",
    "\n",
    "# Index of closest vector\n",
    "closest_index = np.argmin(distances)\n",
    "\n",
    "# Sorted indices by distance\n",
    "sorted_indices = np.argsort(distances)\n",
    "\n",
    "print(\"Closest vector index:\", closest_index)\n",
    "print(\"Distance:\", distances[closest_index])\n",
    "print(\"Sorted indices by distance:\", sorted_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "114ebd89-57e0-41a1-b622-4e7d99e9590c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'🎨🤖 AI & the Soul of Art: Who Really Owns Creativity? — Artists, coders, and philosophers debate the rise of generative art. Can machines make meaning? And where does human intuition still reign supreme?'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "festival_talks[closest_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6972c73a-d373-46cb-828e-bef999f7e7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 🎨🤖 AI & the Soul of Art: Who Really Owns Creativity? — Artists, coders, and philosophers debate the rise of generative art. Can machines make meaning? And where does human intuition still reign supreme?\n",
      "\n",
      "2 🧠 Neural Frontiers: The Brain-Computer Interface Revolution — From thought-controlled devices to memory enhancement, dive into the fast-evolving world of brain tech — and the ethical mazes it brings.\n",
      "\n",
      "3 🪐 Planet B: Terraforming Ideas for Earth 2.0 — Science fiction meets climate urgency. This talk blends real research with wild speculation — from Mars domes to floating cities in the clouds of Venus.\n",
      "\n",
      "4 💼🚫 Post-Work: Imagining a Life Beyond Jobs — As automation reshapes labor, what comes next? UBI, digital nomadism, reputation economies — a candid discussion about freedom, purpose, and survival.\n",
      "\n",
      "5 💻⚖️ Code as Culture: Programming the Future We Want — Code is not neutral — it shapes societies. This talk explores how developers are becoming the new lawmakers, and how we hold them accountable.\n",
      "\n",
      "6 🕶️🌐 Reality is Optional: The Rise of Immersive Worlds — VR, AR, XR — and whatever’s next. What happens when our digital spaces feel more real than reality itself? And who gets to write the rules?\n",
      "\n",
      "7 🕰️🚀 Time Travelers Welcome: Building the Long Now — Futurists, historians, and deep-time thinkers gather to explore projects that think in centuries — from 10,000-year clocks to interstellar archives.\n",
      "\n",
      "8 🕸️🛠️ The Wild Web: Reclaiming the Internet from Algorithms — Can we rebuild the web for people, not profit? Meet the rebels, hackers, and dreamers creating decentralized, community-first digital spaces.\n",
      "\n",
      "9 🌿 The Next Nature: Designing with the Future in Mind — Explore how biomimicry, regenerative design, and synthetic biology are reshaping architecture, materials, and cities. Where does design end and nature begin?\n",
      "\n",
      "10 🧬⏳ The Ethics of Immortality: Living Forever in a Mortal World — Cryonics, gene editing, mind uploading — tech is chasing eternal life. But what would it mean for love, loss, and the human condition?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort festival_talks according to the sorted_indices\n",
    "sorted_festival_talks = [festival_talks[i] for i in sorted_indices]\n",
    "\n",
    "# Optionally print it\n",
    "for num, talk in enumerate(sorted_festival_talks):\n",
    "    print(num+1, talk)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e383a495-fd43-477b-a1b8-cbc3564e446d",
   "metadata": {},
   "source": [
    "So for real retrieval we have to set a retrieval count of how many documents we want to retrieve e.g. **k=3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1fbb3d69-2900-4a1c-8e67-8735443049d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_festival_talks = sorted_festival_talks[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b21c456-8634-4424-985a-7ac54dfb7305",
   "metadata": {},
   "source": [
    "Let's now make our final call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "09dc55df-73e7-4adc-82be-935197ad7273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, based on the retrieved content, here’s a breakdown of potential AI talks at a festival, categorized and with a focus on what might be engaging:\n",
       "\n",
       "**1. Focus on AI & the Soul of Art:**\n",
       "\n",
       "*   **“🎨🤖 AI & the Soul of Art: Who Really Owns Creativity?”** – This is a strong candidate. It directly addresses a significant and evolving topic: the role of AI in creative expression.  It probes the question of meaning and ownership in art – a really compelling conversation.\n",
       "\n",
       "**2. Focus on Brain Tech:**\n",
       "\n",
       "*   **🧠 Neural Frontiers: The Brain-Computer Interface Revolution** – This talk is highly likely to be relevant.  It touches on the core themes of AI and the brain, which is ripe for discussion and potentially offers fascinating insights.\n",
       "\n",
       "**3. Focus on Climate Change & Earth Transformation:**\n",
       "\n",
       "*   **🪐 Planet B: Terraforming Ideas for Earth 2.0** –  This talk, while more speculative, could be captivating. The combination of real science and imaginative world-building makes it potentially highly engaging for a festival audience.\n",
       "\n",
       "\n",
       "---\n",
       "\n",
       "**To help me refine this further and suggest *more* relevant talks, could you tell me:**\n",
       "\n",
       "*   **What kind of festival is it?** (e.g., Tech, Science, Arts, etc.?)\n",
       "*   **What is the overall tone of the festival?** (e.g., Serious and academic, lighthearted and exploratory?)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retrieved_content = '\\n\\n'.join(retrieved_festival_talks)\n",
    "\n",
    "chat_response = chat(model='gemma3:1b', messages=[\n",
    "    {\n",
    "    'role': 'user',\n",
    "    'content': 'User Question:\\nI am interested in AI talks\\n\\n' + 'Answer the user question on festival talks as best as you can based on the retrieved festival talks\\n\\nRetrieved content:\\n\\n' + retrieved_content,\n",
    "    },\n",
    "])\n",
    "\n",
    "display(Markdown(chat_response.message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f448f766-c038-4602-a8e0-2a2f3116562a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Okay we saw that the whole idea of retrieval augmented generation is quite simple and depends on augmenting the LLM with extra context in the prompt. The context could also be retrieved if there is a lot of content and we are not able to just stick everything in the prompt. Now we already have a nice RAG chatbot coded by hand!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709875e1-3e04-43a4-93ea-dc0e7b55f868",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
