{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "625ae3bd-74de-4550-97e2-28dc0c073516",
   "metadata": {},
   "source": [
    "# First steps to becoming a Generative AI Engineer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89c4839-e7cb-4094-80dd-8ed55e3f2968",
   "metadata": {},
   "source": [
    "Large language models are systems that receive input (historically only text but nowadays all kind of modalities) and create output. \n",
    "\n",
    "But that is what basically every system does, what makes LLMs special is that they are able to create much more sophisticated outputs than any other system currently on the market. At least in a lot of use-cases.\n",
    "\n",
    "So let's set some of them up and try them out:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9a6f01-9d99-4d6f-ab26-fdf560410096",
   "metadata": {},
   "source": [
    "## Running LLMs locally with ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aad69f-4b3f-4fee-96fa-6d0c575ddf39",
   "metadata": {},
   "source": [
    "For this course we assume that you have downloaded ollama locally and pulled/ran \"ollama run gemma3:1b\" once to fetch the smallest gemma 3 model. This model is particularly small that it enables fast iteration locally. It not super smart, but so what :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df8e900a-5d92-4505-b695-7c315075ebf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ollama\n",
      "  Obtaining dependency information for ollama from https://files.pythonhosted.org/packages/33/3f/164de150e983b3a16e8bf3d4355625e51a357e7b3b1deebe9cc1f7cb9af8/ollama-0.4.8-py3-none-any.whl.metadata\n",
      "  Downloading ollama-0.4.8-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting httpx<0.29,>=0.27 (from ollama)\n",
      "  Obtaining dependency information for httpx<0.29,>=0.27 from https://files.pythonhosted.org/packages/2a/39/e50c7c3a983047577ee07d2a9e53faf5a69493943ec3f6a384bdc792deb2/httpx-0.28.1-py3-none-any.whl.metadata\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting pydantic<3.0.0,>=2.9.0 (from ollama)\n",
      "  Obtaining dependency information for pydantic<3.0.0,>=2.9.0 from https://files.pythonhosted.org/packages/b0/1d/407b29780a289868ed696d1616f4aad49d6388e5a77f567dcd2629dcd7b8/pydantic-2.11.3-py3-none-any.whl.metadata\n",
      "  Downloading pydantic-2.11.3-py3-none-any.whl.metadata (65 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m65.2/65.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: anyio in /Users/user001/anaconda3/lib/python3.11/site-packages (from httpx<0.29,>=0.27->ollama) (3.5.0)\n",
      "Requirement already satisfied: certifi in /Users/user001/anaconda3/lib/python3.11/site-packages (from httpx<0.29,>=0.27->ollama) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/user001/anaconda3/lib/python3.11/site-packages (from httpx<0.29,>=0.27->ollama) (1.0.5)\n",
      "Requirement already satisfied: idna in /Users/user001/anaconda3/lib/python3.11/site-packages (from httpx<0.29,>=0.27->ollama) (3.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/user001/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama) (0.14.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.9.0->ollama)\n",
      "  Obtaining dependency information for annotated-types>=0.6.0 from https://files.pythonhosted.org/packages/78/b6/6307fbef88d9b5ee7421e68d78a9f162e0da4900bc5f5793f6d3d0e34fb8/annotated_types-0.7.0-py3-none-any.whl.metadata\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.1 (from pydantic<3.0.0,>=2.9.0->ollama)\n",
      "  Obtaining dependency information for pydantic-core==2.33.1 from https://files.pythonhosted.org/packages/d6/7f/c6298830cb780c46b4f46bb24298d01019ffa4d21769f39b908cd14bbd50/pydantic_core-2.33.1-cp311-cp311-macosx_10_12_x86_64.whl.metadata\n",
      "  Downloading pydantic_core-2.33.1-cp311-cp311-macosx_10_12_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-extensions>=4.12.2 (from pydantic<3.0.0,>=2.9.0->ollama)\n",
      "  Obtaining dependency information for typing-extensions>=4.12.2 from https://files.pythonhosted.org/packages/8b/54/b1ae86c0973cc6f0210b53d508ca3641fb6d0c56823f288d108bc7ab3cc8/typing_extensions-4.13.2-py3-none-any.whl.metadata\n",
      "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.9.0->ollama)\n",
      "  Obtaining dependency information for typing-inspection>=0.4.0 from https://files.pythonhosted.org/packages/31/08/aa4fdfb71f7de5176385bd9e90852eaf6b5d622735020ad600f2bab54385/typing_inspection-0.4.0-py3-none-any.whl.metadata\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/user001/anaconda3/lib/python3.11/site-packages (from anyio->httpx<0.29,>=0.27->ollama) (1.2.0)\n",
      "Downloading ollama-0.4.8-py3-none-any.whl (13 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.11.3-py3-none-any.whl (443 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m443.6/443.6 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.33.1-cp311-cp311-macosx_10_12_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: typing-extensions, annotated-types, typing-inspection, pydantic-core, httpx, pydantic, ollama\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.12.1\n",
      "    Uninstalling typing_extensions-4.12.1:\n",
      "      Successfully uninstalled typing_extensions-4.12.1\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.26.0\n",
      "    Uninstalling httpx-0.26.0:\n",
      "      Successfully uninstalled httpx-0.26.0\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.8\n",
      "    Uninstalling pydantic-1.10.8:\n",
      "      Successfully uninstalled pydantic-1.10.8\n",
      "Successfully installed annotated-types-0.7.0 httpx-0.27.0 ollama-0.4.8 pydantic-2.11.3 pydantic-core-2.33.1 typing-extensions-4.11.0 typing-inspection-0.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe4056e7-870b-4a6b-a2d8-23f1e1ab7d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey there! How's your day going? üòä \n",
      "\n",
      "Is there anything you‚Äôd like to chat about or anything I can help you with today?\n"
     ]
    }
   ],
   "source": [
    "from ollama import chat\n",
    "\n",
    "response = chat(model='gemma3:1b', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Hey you!',\n",
    "  },\n",
    "])\n",
    "# notice your exact answer could be different due to some random processes in the model (learn about that later)\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202e6497-0d5a-4d19-9cfe-5b4ba54cb591",
   "metadata": {},
   "source": [
    "As we can see we use the ollama python library to send messages to the ollama model (in our case gemma3:1b). The messages are a list of python dictionaries with each a role and content key. The role is only user and assitant for now where we obviously use user for user messages and assistant for the llm. The content is the generated text/content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253f3420-ade5-43ca-b5ae-d0c3eb6ff453",
   "metadata": {},
   "source": [
    "Let's try another one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c41b03f-c877-4a39-bd42-02a38339be85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That‚Äôs fantastic to hear, Oliver! It‚Äôs great to know you‚Äôre excited about the course. OpenCampus is a really impressive platform, and I'm glad you're taking the plunge.\n",
      "\n",
      "It‚Äôs wonderful that you‚Äôre participating. What specifically are you hoping to get out of the course? Do you have any particular areas you're interested in exploring or learning more about? üòä\n"
     ]
    }
   ],
   "source": [
    "response_2 = chat(model='gemma3:1b', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Hey I am Oliver and I am happy to participate in this exciting new LLM and AI agents course at OpenCampus!',\n",
    "  },\n",
    "])\n",
    "\n",
    "print(response_2.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e37213d-3583-414a-aca2-83c5024bd1bb",
   "metadata": {},
   "source": [
    "Oh that so niceüòä Let's see if it remembers me!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d87ceb79-58b4-4824-a23c-86430dcc92c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am an AI assistant designed to help you with a wide variety of tasks! I don‚Äôt have access to personal information, so I need you to tell me a bit about yourself. üòä\n",
      "\n",
      "To help me figure out who you are, could you tell me:\n",
      "\n",
      "*   **What are you doing today?** (e.g., writing a story, answering questions, generating creative content?)\n",
      "*   **What's your general topic of interest?** (e.g., science, history, art, gaming, etc.)\n",
      "*   **Are you a person?** (e.g., a friend, a family member, or someone you've just met?)\n"
     ]
    }
   ],
   "source": [
    "response_3 = chat(model='gemma3:1b', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Who am I?',\n",
    "  },\n",
    "])\n",
    "\n",
    "print(response_3.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51212af8-62a6-4acd-8b04-e74407bc1cf7",
   "metadata": {},
   "source": [
    "Oh it forget completely who I was. That's so bad.üò≠\n",
    "\n",
    "The reason is that the model does not update its weights (i.e. its neuronal connections) permanentially for each of my inputs. So after it produced the answer everything is reset and hence any memory of me is lost.üíÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20036c7d-9687-4ce1-9bcd-b8e05704ac67",
   "metadata": {},
   "source": [
    "So to keep the conversation going we have to put all the conversation messages into the messages list in the correct order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8948d57a-9533-4db2-84af-1a35edf8198f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are **Oliver**. üòä \n",
      "\n",
      "It was a fun little guessing game! üòÑ\n"
     ]
    }
   ],
   "source": [
    "chat_response = chat(model='gemma3:1b', messages=[\n",
    "    {\n",
    "    'role': 'user',\n",
    "    'content': 'Hey I am Oliver and I am happy to participate in this exciting new llm and ai agents course at opencampus!',\n",
    "    },\n",
    "    {\n",
    "    'role': 'assistant',\n",
    "    'content': '''Hey Oliver! That‚Äôs fantastic to hear! It‚Äôs great you‚Äôre excited about the course. OpenCampus is a really cool platform, and it‚Äôs awesome you‚Äôre diving in.\\n\\nIt‚Äôs great to connect with you too. What specifically are you hoping to get out of the course? Are you interested in learning about LLMs, AI agents, or something else entirely?\\n\\nI'm happy to chat about it ‚Äì feel free to tell me anything you're thinking. üòä''',\n",
    "    },\n",
    "    {\n",
    "    'role': 'user',\n",
    "    'content': 'Who am I?',\n",
    "    },\n",
    "])\n",
    "\n",
    "print(chat_response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e633c5a8-26f5-45a8-8dae-e1462f74153d",
   "metadata": {},
   "source": [
    "So now it now I remembers me!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e2b299-d8fe-4644-bf51-7698f74a1f42",
   "metadata": {},
   "source": [
    "Ok, let's now do some friendly teasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ab67126-0b8b-402f-97b6-ed9da10e231d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, I understand you want me to rephrase something. Please tell me what you want me to rephrase.\n"
     ]
    }
   ],
   "source": [
    "tease_response = chat(model='gemma3:1b', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 3000*'Hi',\n",
    "  },\n",
    "])\n",
    "\n",
    "print(tease_response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb82e3cf-f374-44c7-a5ce-11990aeaab5c",
   "metadata": {},
   "source": [
    "As you can see our AI friend is still a bit stupid at some inputs which it has not encountered during training. 3000 \"Hi\"s were recognized as an image..can you maybe think of why? :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddea7da-4bf4-4bbd-8c01-fdf3be788d89",
   "metadata": {},
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf2372b-609c-4485-a900-fdc908bb1c02",
   "metadata": {},
   "source": [
    "Be creative and try to fool/tease the model with inputs it was probably not trained on and see what happensüòä"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b781174b-3b23-4156-a8c9-3c4d8aaab1d3",
   "metadata": {},
   "source": [
    "## Using OpenAI LLMs\n",
    "\n",
    "Let's now try out the models from the company which generated to original buzz around LLMs!\n",
    "(You need your own OPENAI API KEY to play around which this section)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adec23d0-94d3-4620-8bfe-6115202fc46a",
   "metadata": {},
   "source": [
    "We can use OpenAIs LLMs via two ways:\n",
    "- OpenAI API\n",
    "- Microsoft Azure API\n",
    "\n",
    "So now we using these models as services i.e. they run on somebodyelse computers and we pay for them either with our data or with dollars or with both. Just be aware of it that what your inputting is now transferred to a third-party."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9fecf01b-5a83-497b-990a-1b4c91c4c75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Users/user001/anaconda3/lib/python3.11/site-packages (1.17.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/user001/anaconda3/lib/python3.11/site-packages (from openai) (3.5.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/user001/anaconda3/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/user001/anaconda3/lib/python3.11/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/user001/anaconda3/lib/python3.11/site-packages (from openai) (2.11.3)\n",
      "Requirement already satisfied: sniffio in /Users/user001/anaconda3/lib/python3.11/site-packages (from openai) (1.2.0)\n",
      "Requirement already satisfied: tqdm>4 in /Users/user001/anaconda3/lib/python3.11/site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/user001/anaconda3/lib/python3.11/site-packages (from openai) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/user001/anaconda3/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: certifi in /Users/user001/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/user001/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/user001/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/user001/anaconda3/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /Users/user001/anaconda3/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/user001/anaconda3/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcad52f-aa07-4d34-a41e-0ea65b107324",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mspawn /Users/user001/opt/anaconda3/bin/conda ENOENT. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "# replace the random example string with your own key, take care not to share this file with others - otherwise they can use your ressoures\n",
    "\n",
    "openai_api_key = \"< Enter your OpenAI API key here >\"\n",
    "\n",
    "client = OpenAI(api_key = openai_api_key)\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a one-sentence bedtime story about a unicorn.\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314c3451-f6d0-4101-b9d9-f01907202316",
   "metadata": {},
   "source": [
    "So as we can see, the general setup is similar, we also have the messages list. We only need now an API Key to use the external services and have different names from the library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf1431d-f7b2-4428-b48f-5b506e74eff2",
   "metadata": {},
   "source": [
    "## Using the free HuggingFace API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be37c4a5-f87a-4ea2-a71b-3e8819274532",
   "metadata": {},
   "source": [
    "There is also a free api from HuggingFace which allows you to test your code which models which are freely available. You just need to setup a HuggingFace Account and a READ token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64d3cc2-2df9-4f51-8d83-9d26a4f39a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': 'Alright, let\\'s tackle this problem step by step. The question is asking: \\n\\n**\"Which number yields the same result when it is added or multiplied with itself?\"**\\n\\nAt first glance, this seems straightforward, but let\\'s break it down to ensure we understand it correctly and arrive at the right answer.\\n\\n### Understanding the Problem\\n\\nWe need to find a number, let\\'s call it **x**, such that:\\n\\n- When you add **x** to itself: **x + x**\\n- When you multiply **x** by itself: **x * x**\\n\\nThese two operations should give the same result. In other words:\\n\\n**x + x = x * x**\\n\\n### Translating to an Equation\\n\\nLet\\'s write this as an equation:\\n\\nx + x = x * x\\n\\nSimplifying the left side:\\n\\n2x = x¬≤\\n\\nNow, we have a simple quadratic equation:\\n\\nx¬≤ = 2x\\n\\n### Solving the Equation\\n\\nTo solve for x, let\\'s rearrange the equation:\\n\\nx¬≤ - 2x = 0\\n\\nThis can be factored as:\\n\\nx(x - 2) = 0\\n\\nSetting each factor equal to zero gives us the solutions:\\n\\n1. x = 0\\n2. x - 2 = 0 ‚áí x = 2\\n\\nSo, the possible solutions are x = 0 and x = 2.\\n\\n### Verifying the Solutions\\n\\nIt\\'s always good practice to verify our solutions by plugging them back into the original conditions.\\n\\n**First Solution: x = 0**\\n\\n- Addition: 0 + 0 = 0\\n- Multiplication: 0 * 0 = 0\\n- Comparison: 0 = 0 ‚úîÔ∏è\\n\\n**Second Solution: x = 2**\\n\\n- Addition: 2 + 2 = 4\\n- Multiplication: 2 * 2 = 4\\n- Comparison: 4 = 4 ‚úîÔ∏è\\n\\nBoth 0 and 2 satisfy the original condition.\\n\\n### Considering the Context\\n\\nNow, the question asks for \"which number,\" which might imply a single answer. However, mathematically, both 0 and 2 satisfy the given condition. \\n\\nBut let\\'s think about the phrasing: \"the same result when it is added or multiplied with itself.\" \\n\\n- For 0: Adding or multiplying by itself both yield 0.\\n- For 2: Adding or multiplying by itself both yield 4.\\n\\nBoth are valid, but if we\\'re to choose one, often such puzzles expect the non-trivial solution (i.e., not zero), unless specified otherwise.\\n\\n### Potential Missteps\\n\\nInitially, one might think only of positive numbers or overlook zero. It\\'s easy to jump to 2 because it\\'s a small positive integer that fits, but zero is also a valid number that satisfies the condition. \\n\\nAnother possible confusion could be interpreting \"added or multiplied with itself\" differently, but the straightforward interpretation is correct here.\\n\\n### Final Answer\\n\\nAfter carefully working through the problem, the numbers that satisfy the condition are:\\n\\n**0 and 2**\\n\\nHowever, if we\\'re to pick one, the more commonly expected answer is:\\n\\n**2**\\n\\nBecause:\\n\\n- 2 + 2 = 4\\n- 2 * 2 = 4\\n\\nBoth operations yield the same result, which is 4. Zero also works, but it\\'s a trivial case where adding or multiplying zero by itself always gives zero.\\n\\n### Conclusion\\n\\nThrough setting up the equation based on the given condition and solving it, we found that both 0 and 2 satisfy the requirement where adding the number to itself is the same as multiplying the number by itself. Depending on the context, either could be the intended answer, but typically, 2 is the number that comes to mind first in such puzzles. \\n\\n**Final Answer:** The number is **2**.'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://router.huggingface.co/novita/v3/openai/chat/completions\"\n",
    "# replace the random example string with your own key, take care not to share this file with others - otherwise the can use your ressoures\n",
    "headers = {\"Authorization\": \"Bearer <Enter your Hugging Face Access Token here>\"}\n",
    "payload = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Which number yields the same result when it is added or multiplied with itself?\"\n",
    "        }\n",
    "    ],\n",
    "    \"model\": \"deepseek/deepseek-v3-0324\",\n",
    "}\n",
    "\n",
    "response = requests.post(API_URL, headers=headers, json=payload)\n",
    "print(response.json()[\"choices\"][0][\"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5a733e72-b349-48a2-a5b8-20eabe8b1058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Alright, let's tackle this problem step by step. The question is asking: \n",
       "\n",
       "**\"Which number yields the same result when it is added or multiplied with itself?\"**\n",
       "\n",
       "At first glance, this seems straightforward, but let's break it down to ensure we understand it correctly and arrive at the right answer.\n",
       "\n",
       "### Understanding the Problem\n",
       "\n",
       "We need to find a number, let's call it **x**, such that:\n",
       "\n",
       "- When you add **x** to itself: **x + x**\n",
       "- When you multiply **x** by itself: **x * x**\n",
       "\n",
       "These two operations should give the same result. In other words:\n",
       "\n",
       "**x + x = x * x**\n",
       "\n",
       "### Translating to an Equation\n",
       "\n",
       "Let's write this as an equation:\n",
       "\n",
       "x + x = x * x\n",
       "\n",
       "Simplifying the left side:\n",
       "\n",
       "2x = x¬≤\n",
       "\n",
       "Now, we have a simple quadratic equation:\n",
       "\n",
       "x¬≤ = 2x\n",
       "\n",
       "### Solving the Equation\n",
       "\n",
       "To solve for x, let's rearrange the equation:\n",
       "\n",
       "x¬≤ - 2x = 0\n",
       "\n",
       "This can be factored as:\n",
       "\n",
       "x(x - 2) = 0\n",
       "\n",
       "Setting each factor equal to zero gives us the solutions:\n",
       "\n",
       "1. x = 0\n",
       "2. x - 2 = 0 ‚áí x = 2\n",
       "\n",
       "So, the possible solutions are x = 0 and x = 2.\n",
       "\n",
       "### Verifying the Solutions\n",
       "\n",
       "It's always good practice to verify our solutions by plugging them back into the original conditions.\n",
       "\n",
       "**First Solution: x = 0**\n",
       "\n",
       "- Addition: 0 + 0 = 0\n",
       "- Multiplication: 0 * 0 = 0\n",
       "- Comparison: 0 = 0 ‚úîÔ∏è\n",
       "\n",
       "**Second Solution: x = 2**\n",
       "\n",
       "- Addition: 2 + 2 = 4\n",
       "- Multiplication: 2 * 2 = 4\n",
       "- Comparison: 4 = 4 ‚úîÔ∏è\n",
       "\n",
       "Both 0 and 2 satisfy the original condition.\n",
       "\n",
       "### Considering the Context\n",
       "\n",
       "Now, the question asks for \"which number,\" which might imply a single answer. However, mathematically, both 0 and 2 satisfy the given condition. \n",
       "\n",
       "But let's think about the phrasing: \"the same result when it is added or multiplied with itself.\" \n",
       "\n",
       "- For 0: Adding or multiplying by itself both yield 0.\n",
       "- For 2: Adding or multiplying by itself both yield 4.\n",
       "\n",
       "Both are valid, but if we're to choose one, often such puzzles expect the non-trivial solution (i.e., not zero), unless specified otherwise.\n",
       "\n",
       "### Potential Missteps\n",
       "\n",
       "Initially, one might think only of positive numbers or overlook zero. It's easy to jump to 2 because it's a small positive integer that fits, but zero is also a valid number that satisfies the condition. \n",
       "\n",
       "Another possible confusion could be interpreting \"added or multiplied with itself\" differently, but the straightforward interpretation is correct here.\n",
       "\n",
       "### Final Answer\n",
       "\n",
       "After carefully working through the problem, the numbers that satisfy the condition are:\n",
       "\n",
       "**0 and 2**\n",
       "\n",
       "However, if we're to pick one, the more commonly expected answer is:\n",
       "\n",
       "**2**\n",
       "\n",
       "Because:\n",
       "\n",
       "- 2 + 2 = 4\n",
       "- 2 * 2 = 4\n",
       "\n",
       "Both operations yield the same result, which is 4. Zero also works, but it's a trivial case where adding or multiplying zero by itself always gives zero.\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "Through setting up the equation based on the given condition and solving it, we found that both 0 and 2 satisfy the requirement where adding the number to itself is the same as multiplying the number by itself. Depending on the context, either could be the intended answer, but typically, 2 is the number that comes to mind first in such puzzles. \n",
       "\n",
       "**Final Answer:** The number is **2**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Uhh let's render this long markdown string to read it more easily\n",
    "from IPython.display import Markdown, display\n",
    "display(Markdown(response.json()[\"choices\"][0][\"message\"][\"content\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07533d46-4b85-439d-a6f9-a50bb2a66cd8",
   "metadata": {},
   "source": [
    "Really cool, isn't it? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20558ca8-a69c-497c-89cc-07d426319300",
   "metadata": {},
   "source": [
    "### Short excurse about the term payload\n",
    "\n",
    "I wondered always why they use the term **payload**. So here is a nice explanation: \n",
    "\n",
    "In API requests, the term payload refers to the **actual data** or information being sent within the request, typically in the body of an HTTP request (such as POST or PUT). The term is derived from communication and networking protocols, where the payload is the part of transmitted data that contains the meaningful information, as opposed to headers or metadata which describe or route the data.\n",
    "\n",
    "Why is the term \"payload\" commonly used?\n",
    "\n",
    "Historical Origin:\n",
    "The term originally comes from aerospace and transport, describing the **actual useful content** carried by a vehicle (like cargo in a truck or spacecraft).\n",
    "\n",
    "Clear Differentiation:\n",
    "It clearly distinguishes between control data (headers, parameters, routing information) and the actual content (payload) intended for processing by the receiving application or service.\n",
    "\n",
    "Generalization:\n",
    "Payload provides a generic, neutral term for the transmitted data, regardless of its structure (JSON, XML, plain text, binary files, etc.), which simplifies communication across different APIs and systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48ea1af-5f07-4344-a762-faf96d4ab47f",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "So there are a lot different LLMs out in the wild. They differ in size and capabilities. We always access them via an API call. They run either locally, in our own cloud or we use some third party provider. Always check where the data is going and what implactions that might have!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8b24c5-ac06-4f70-86e3-295149469714",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
